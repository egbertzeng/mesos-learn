
####################################################################################################################################
一、修改机器信息
####################################################################################################################################
0.关闭防火墙
    systemctl stop firewalld.service 
    systemctl disable firewalld.service 
1.修改密码
    passwd root
    密码修改为12345678

2.修改机器名称
    vim /etc/sysconfig/network
    写入 ：HOSTNAME=bigdata03
    重启机器：reboot -h now 
    验证：hostname 

    此语句重启无效
    echo bigdata05 > /proc/sys/kernel/hostname

    不用重启机器就能看到修改的办法
     hostnamectl --static set-hostname master

3.配置hosts文件
    vim /etc/hosts
    写入
    10.100.134.1 gateway   
    10.100.134.2 cluster   
    10.100.134.3 bigdata03
    10.100.134.4 bigdata04
    10.100.134.5 bigdata05
    分发配置文件
    scp /etc/hosts bigdata04:/etc/hosts
    scp /etc/hosts bigdata05:/etc/hosts


4.配置集群免密登陆
    各个机器上生产新的秘钥
    rm -rf /root/.ssh/*
    ssh-keygen -t rsa
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

    汇聚到bigdata03
    ssh-copy-id -i  bigdata03

    分发到其他机器
    scp /root/.ssh/authorized_keys bigdata04:/root/.ssh/
    scp /root/.ssh/authorized_keys bigdata05:/root/.ssh/


####################################################################################################################################
二、Java部署
####################################################################################################################################

下载目录
/cloudstar/software
下载jdk
    wget http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz
解压JDK
	tar -zxvf jdk-8u144-linux-x64.tar.gz
分发JDK
    scp -r jdk1.8.0_144 bigdata04:/cloudstar/software/
    scp -r jdk1.8.0_144 bigdata05:/cloudstar/software/

  配置环境变量
     vim ~/.bashrc 
     写入
		export JAVA_HOME=/cloudstar/software/jdk1.8.0_144
		PATH=${JAVA_HOME}/bin:$PAHT:
     刷新环境变量
     source ~/.bashrc 

    分发环境变量
        scp  ~/.bashrc   bigdata04:~/.bashrc 
        scp  ~/.bashrc   bigdata05:~/.bashrc 

验证安装成功
        命令：java -version
        回显：
        java version "1.8.0_74"
        Java(TM) SE Runtime Environment (build 1.8.0_74-b02)
        Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)


####################################################################################################################################
三、zookeeper部署
####################################################################################################################################

规划和策略：
    bigdata03,bigdata04,bigdata05，上部署zookeeper
    在bigdata03上配置，配置完成后发送到其他机器上
下载zookeeper
    wget http://apache.fayea.com/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz
解压zookeeper
     tar -zxvf zookeeper-3.4.10.tar.gz

配置环境变量
 vim ~/.bashrc 
 写入
	ZOOKEEPER_HOME=/cloudstar/software/zookeeper-3.4.10
	PATH=$ZOOKEEPER_HOME/bin$PATH:
 刷新环境变量
 source ~/.bashrc 
分发环境变量
    scp  ~/.bashrc   bigdata04:~/.bashrc 
    scp  ~/.bashrc   bigdata05:~/.bashrc 


配置zookeeper
    1.修改配置文件
         cd  ${ZOOKEEPER_HOME}/conf
         cp zoo_sample.cfg zoo.cfg
         vim ${ZOOKEEPER_HOME}/conf/zoo.cfg
         修改项如下：
         a.指定数据文件的存放位置(同步数据存放的位置)
             dataDir=/cloudstar/software/zookeeper-3.4.10/data
         b.增加zookeeper节点：(配置如下信息)
            server.3=bigdata03:2888:3888
            server.4=bigdata04:2888:3888
            server.5=bigdata05:2888:3888
           （2888:数据传输端口,3888:leader和flower的选举端口    ）
    2.创建文件夹和文件
            mkdir /cloudstar/software/zookeeper-3.4.10/data
            echo  3 > /cloudstar/software/zookeeper-3.4.10/data/myid
分发zookeeper
    scp -r zookeeper-3.4.10 bigdata04:/cloudstar/software/
    scp -r zookeeper-3.4.10 bigdata05:/cloudstar/software/

修改对应机器上的id
    echo  4 > $ZOOKEEPER_HOME/data/myid
    echo  5 > $ZOOKEEPER_HOME/data/myid
zookeeper常用命令
    ${ZOOKEEPER_HOME}/bin/zkServer.sh start
    ${ZOOKEEPER_HOME}/bin/zkServer.sh stop
    ${ZOOKEEPER_HOME}/bin/zkServer.sh restart
    ${ZOOKEEPER_HOME}/bin/zkServer.sh status
查看zookeeper日志
    more ${ZOOKEEPER_HOME}/bin/zookeeper.out



####################################################################################################################################
四、docker部署
####################################################################################################################################
安装docker
http://blog.csdn.net/wangfei0904306/article/details/62046753
    1.升级
    	yum -y update 
    1.安装：
        yum-config-manager  --add-repo https://download.docker.com/linux/centos/docker-ce.repo  
		yum makecache fast  
		yum -y install docker-ce 
    2. 启动&开机自启动
        systemctl start docker
        systemctl enable docker
    3.重置加速镜像
		vim  /etc/docker/daemon.json
			{"registry-mirrors": ["http://hub-mirror.c.163.com","https://docker.mirrors.ustc.edu.cn"]}
    4.重启docker服务
		systemctl restart docker
    5.测试最小镜像
        docker pull hello-world
        docker run hello-world


####################################################################################################################################
五、docker私有仓库
####################################################################################################################################

测试私有仓库是否可用
    docker pull hello-world
    docker tag hello-world 10.100.134.3:5000/hello-world
    docker push 10.100.134.3:5000/hello-world

如果出现如下错误
    http: server gave HTTP response to HTTPS client
解决方法如下：
    1.修改配置文件
        vim /etc/docker/daemon.json
        填写如下内容
        {
            "registry-mirrors": ["http://hub-mirror.c.163.com","https://docker.mirrors.ustc.edu.cn","10.100.134.3:5000"],
            "insecure-registries":["10.100.134.3:5000"]
        }

    2.分发配置文件
        scp /etc/docker/daemon.json  bigdata04:/etc/docker/daemon.json
        scp /etc/docker/daemon.json  bigdata05:/etc/docker/daemon.json
    3.重启docker服务
        systemctl restart docker
    4.测试是否成功
        测试命令：
        curl http://10.100.134.3:5000/v2/_catalog
        curl http://10.100.134.3:5000/v2/registry/tags/list
        返回结果：
        {"repositories":["hello-world"]}

####################################################################################################################################
六、mesos部署
####################################################################################################################################
参考链接如下：
    http://www.linuxidc.com/Linux/2017-03/141478.htm
    http://www.xuliangwei.com/xubusi/422.html
    http://www.mamicode.com/info-detail-1948163.html
    http://www.cnblogs.com/ee900222/p/docker_2.html
    https://www.zwbing.com/80.html
    http://blog.csdn.net/felix_yujing/article/details/51813224
零、集群规划

	bigdata03  mesos-master  mesos-slave marathon  chronos
	bigdata04  mesos-slave
	bigdata05  mesos-slave
一、安装mesos-mater
    1.安装mesosphere
		rpm -ivh http://repos.mesosphere.io/el/7/noarch/RPMS/mesosphere-el-repo-7-1.noarch.rpm
		yum -y install mesos marathon chronos
    2.配置zookeeper
        vim /etc/mesos/zk
        zk://bigdata03:2181,bigdata04:2181,bigdata05:2181/mesos
    3.启动服务&开机启动
        systemctl start  mesos-master mesos-slave marathon chronos
        systemctl enable mesos-master mesos-slave marathon  chronos
         
    4.验证启动，webui
        mesos界面：      http://10.100.134.3:5050
        marathon界面：   http://10.100.134.3:8080
        marathon界面：   http://10.100.134.3:4400
二、安装mesos-slave
    1.安装mesosphere
        rpm -ivh http://repos.mesosphere.io/el/7/noarch/RPMS/mesosphere-el-repo-7-1.noarch.rpm
        yum -y install mesos
    2.配置zookeeper
        vim /etc/mesos/zk
        zk://bigdata03:2181,bigdata04:2181,bigdata05:2181/mesos
    3.启动服务&开机启动
        systemctl start  mesos-slave mesos-master
        systemctl enable mesos-slave mesos-master
       
三、运行Mesos任务，可以在Web界面查看task
    MASTER=$(mesos-resolve `cat /etc/mesos/zk`)
    mesos-execute --master=$MASTER --name="cluster-test" --command="sleep 60"

四、让mesos支持docker技术
    1.配置所有mesos-slave
        echo 'docker,mesos' | tee /etc/mesos-slave/containerizers
        echo '100000mins' > /etc/mesos-slave/executor_registration_timeout



echo 'file:///etc/mesos-agent/resources.txt'>/etc/mesos-agent/resources

systemctl stop mesos-slave
systemctl start mesos-slave


    2.重启所有mesos-slave
        systemctl restart mesos-slave

                systemctl restart mesos-master


重要目录
  /usr/etc/mesos
  /var/log/mesos


几个配置启动参数的目录： 
/etc/mesos-master/ 
/etc/mesos-slave/ 
/etc/marathon/conf/ 
在这些目录分别用来配置mesos-master，mesos-slave，marathon的启动参数。以参数名为文件名，参数值为文件内容即可。

服务将被连接到如下path
/etc/systemd/system/multi-user.target.wants

####################################################################################################################################
七、测试marathon
####################################################################################################################################

一、marathon测试案例1
要求：
  部署一个打印语句,输出将在stdout的日志中查看
方案：
{
  "id": "hello",
  "cmd": "while [ true ] ; do echo 'Hello Marathon' >> /cloudstar/hello.txt; sleep 1; done",
  "cpus": 0.01,
  "mem": 1.0,
  "instances": 1
}


二、marathon测试案例2
要求：
  用nc命令启动一个HTTP服务
1.在各节点上安装netcat
    yum install nmap-ncat
2.在Marathon页面，点击“Create Application”创建任务
{
  "id": "netcat",
  "cmd": "while true; do ( echo 'HTTP/1.0 200 Ok'; echo; echo 'Hello World' ) | nc -l $PORT; done",
  "cpus": 0.01,
  "mem": 1.0,
  "instances": 1
}
3.根据Marathon页面提供的端口
    命令
    curl http://bigdata05:31979
    返回
    Hello World 

二、marathon测试案例1
要求：
  用nc命令启动一个HTTP服务
方案：
{
  "id": "ubuntu-marathon",
  "instances": 2,
  "cpus": 0.5,
  "mem": 256,
  "uris": [],
  "cmd": "while sleep 10; do date -u +%T; done",
  "container": {
    "type": "DOCKER",
    "docker": {
    "image": "libmesos/ubuntu"
    }
  }
}


二、marathon部署一套Nginx环境
要求：
Marathon有自己的REST API，我们通过API的方式来创建一个Nginx的Docker容器。

方案：
首先创建如下的配置文件nginx.json
{
	"id":"nginx",
	"cpus":0.2,
	"mem":32.0,
	"instances": 1,
	"constraints": [["hostname","UNIQUE",""]],
	"container": {
		"type":"DOCKER",
		"docker": {
			"image": "nginx",
			"network": "BRIDGE",
			"portMappings": [
			 {
				"containerPort": 80,
				"hostPort": 0,
				"servicePort": 0,
				 "protocol":"tcp" 
			 }
			]
		  }
	},
    "healthChecks": [
    {
      "protocol": "HTTP",
      "portIndex": 0,
      "path": "/",
      "gracePeriodSeconds": 5,
      "intervalSeconds": 20,
      "maxConsecutiveFailures": 3
    }
    ] 
}

三、marathon部署一套Tomcat环境（精简代码）
{  
	"id":"tomcat",  
	"cpus":1,  
	"mem":128,  
	"instances": 1,  
	"constraints": [["hostname", "UNIQUE",""]],  
	"container": {  
		"type":"DOCKER",  
		"docker": {  
			"image": "tomcat",                                              
			"network": "BRIDGE", 
			"portMappings": [  
			{"containerPort": 8080, "hostPort": 31001,"servicePort": 31002, "protocol": "tcp" }]  
		}  
	},
    "healthChecks": [
    {
      "protocol": "HTTP",
      "portIndex": 0,
      "path": "/",
      "gracePeriodSeconds": 5,
      "intervalSeconds": 20,
      "maxConsecutiveFailures": 3
    }
  ]
}



三、marathon部署一套Tomcat环境(完整代码)
{
  "id": "/test001/tomcat",
  "cmd": null,
  "cpus": 0.1,
  "mem": 128,
  "disk": 0,
  "instances": 3,
  "constraints": [
    [
      "hostname",
      "UNIQUE",
      ""
    ]
  ],
  "acceptedResourceRoles": [
    "*"
  ],
  "container": {
    "type": "DOCKER",
    "volumes": [],
    "docker": {
      "image": "tomcat",
      "network": "BRIDGE",
      "portMappings": [
        {
          "containerPort": 8080,
          "hostPort": 31091,
          "servicePort": 31002,
          "protocol": "tcp",
          "labels": {}
        }
      ],
      "privileged": false,
      "parameters": [],
      "forcePullImage": false
    }
  },
  "healthChecks": [
    {
      "protocol": "HTTP",
      "portIndex": 0,
      "path": "/",
      "gracePeriodSeconds": 5,
      "intervalSeconds": 20,
      "maxConsecutiveFailures": 3
    }
  ],
  "portDefinitions": [
    {
      "port": 31002,
      "protocol": "tcp",
      "name": "default",
      "labels": {}
    }
  ]
}

四、marathon部署一套pyhton3的web程序
{
  "id": "bridged-webapp",
  "cmd": "python3 -m http.server 8080",
  "cpus": 0.5,
  "mem": 64.0,
  "instances": 2,
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "python:3",
      "network": "BRIDGE",
      "portMappings": [
        { "containerPort": 8080, "hostPort": 0, "servicePort": 9000, "protocol": "tcp" },
        { "containerPort": 161, "hostPort": 0, "protocol": "udp"}
      ]
    }
  },
  "healthChecks": [
    {
      "protocol": "HTTP",
      "portIndex": 0,
      "path": "/",
      "gracePeriodSeconds": 5,
      "intervalSeconds": 20,
      "maxConsecutiveFailures": 3
    }
  ]
}



五、marathon部署一套前端代码（可选测试）

1.部署前端代码到marathon(随机端口映射)

{
    "id":"/telecomapistore/portal-service",
    "cpus":0.6,
    "mem":120.0,
    "instances": 3,
    "constraints": [["hostname","UNIQUE",""]],
    "container": {
        "type":"DOCKER",
        "docker": {
            "image": "10.100.134.3:5000/telecomapistore/portal-service",
            "network": "BRIDGE",
            "portMappings": [{ "containerPort": 80,"hostPort": 0,"servicePort": 0, "protocol":"tcp" }]
          }
    } 
}

2.部署前端代码到marathon(指定端口映射)

{
    "id":"/telecomapistore/portal-service",
    "cpus":0.6,
    "mem":600.0,
    "instances": 3,
    "constraints": [["hostname","UNIQUE",""]],
    "container": {
        "type":"DOCKER",
        "docker": {
            "image": "10.100.134.3:5000/telecomapistore/portal-service",
            "network": "BRIDGE",
            "forcePullImage": true,
            "portMappings": [{"containerPort": 80, "hostPort": 31111,"servicePort": 0, "protocol": "tcp" }]  
          }
    } 
}

####################################################################################################################################
####################################################################################################################################
####################################################################################################################################
####################################################################################################################################
####################################################################################################################################
####################################################################################################################################
####################################################################################################################################
####################################################################################################################################

####################################################################################################################################
一、大数据产品线前端部署方案
####################################################################################################################################

==================================================================
一、部署telecomapistore/portal-service的方案
==================================================================

1.编译前端代码到dist
    npm build

2.将dist打包成docker镜像
    docker-compose build

3.推送镜像到私用仓库
    docker tag telecomapistore/portal-service  10.100.134.3:5000/telecomapistore/portal-service
    docker push 10.100.134.3:5000/telecomapistore/portal-service

4.用marathon拉取仓库中的镜像，并部署到集群中
{
    "id":"/telecomapistore/portal-service",
    "cpus":0.6,
    "mem":600.0,
    "instances": 3,
    "constraints": [["hostname","UNIQUE",""]],
    "container": {
        "type":"DOCKER",
        "docker": {
            "image": "10.100.134.3:5000/telecomapistore/portal-service",
            "network": "BRIDGE",
            "forcePullImage": true,
            "portMappings": [{"containerPort": 80, "hostPort": 31111,"servicePort": 0, "protocol": "tcp" }]  
          }
    } 
}

5.如果不用第4步也可以人肉部署仓库中的镜像
    docker pull 10.100.134.3:5000/telecomapistore/portal-service
    docker kill portal-service
    docker rm  portal-service
    docker run -d -p 31111:80 --name  portal-service  10.100.134.3:5000/telecomapistore/portal-service

==================================================================
二、部署telecomapistore/portal-service的方案
==================================================================
1.编译前端代码到dist
    npm build

2.将dist打包成docker镜像
    docker-compose build

3.推送镜像到私用仓库
    docker tag marketing/portal-service  10.100.134.3:5000/marketing/portal-service
    docker push 10.100.134.3:5000/marketing/portal-service
4.用marathon拉取仓库中的镜像，并部署到集群中
{
    "id":"/marketing/portal-service",
    "cpus":0.6,
    "mem":600.0,
    "instances": 3,
    "constraints": [["hostname","UNIQUE",""]],
    "container": {
        "type":"DOCKER",
        "docker": {
            "image": "10.100.134.3:5000/marketing/portal-service",
            "network": "BRIDGE",
            "forcePullImage": true,
            "portMappings": [{"containerPort": 80, "hostPort": 31112,"servicePort": 0, "protocol": "tcp" }]  
          }
    } 
}
5.如果不用第4步也可以人肉部署仓库中的镜像（参考其他镜像的人肉部署方式）





####################################################################################################################################
二、大数据产品线后端部署方案
####################################################################################################################################
==================================================================
一、部署telecomapistore/portal-service的方案
==================================================================




从私有仓库部署到marathon

docker rmi telecomapistore/eureka-service -f
docker rmi 10.100.134.3:5000/telecomapistore/eureka-service -f
docker tag telecomapistore/eureka-service 10.100.134.3:5000/telecomapistore/eureka-service
docker push 10.100.134.3:5000/telecomapistore/eureka-service
docker pull 10.100.134.3:5000/telecomapistore/eureka-service



{
    "id":"/telecomapistore/eureka-service",
    "mem":256,
    "instances":1,
    "container":{
        "type":"DOCKER",
        "docker":{
           "network": "BRIDGE",
            "forcePullImage": true,
            "image":"10.100.134.3:5000/telecomapistore/eureka-service",
            "portMappings":[
                {
                    "containerPort":8761,
                    "hostPort":0,
                    "servicePort":0,
                    "protocol":"tcp",
                    "name":"http"
                }
            ]
        }
    },
    "env":{
        "spring.profiles.active":"dev",
        "ALL_SERVICE_RUN_HOST":"10.100.134.3",
        "EUREKA_SERVER_PORT":"8761"
    }
}






**************************
**************************
**************************
**************************
**************************








**************************
**************************
**************************
**************************
**************************








docker rmi telecomapistore/eureka-service -f
docker rmi 10.100.134.3:5000/telecomapistore/eureka-service -f
docker tag telecomapistore/eureka-service 10.100.134.3:5000/telecomapistore/eureka-service
docker push 10.100.134.3:5000/telecomapistore/eureka-service
docker pull 10.100.134.3:5000/telecomapistore/eureka-service

docker rm eureka-service
docker run -p 8761:8761 --name  eureka-service  10.100.134.3:5000/telecomapistore/eureka-service

docker rm eureka-service
docker run -p 31001:31001 --name  eureka-service  10.100.134.3:5000/telecomapistore/eureka-service




docker rmi 10.100.134.3:5000/telecomapistore/configuration-service
docker tag telecomapistore/configuration-service 10.100.134.3:5000/telecomapistore/configuration-service
docker push 10.100.134.3:5000/telecomapistore/configuration-service
docker pull 10.100.134.3:5000/telecomapistore/configuration-service


{
    "id":"/telecomapistore/configuration-service",
    "mem":256,
    "instances":1,
    "container":{
        "type":"DOCKER",
        "docker":{
            "network": "HOST",
            "forcePullImage": true,
            "image":"10.100.134.3:5000/telecomapistore/configuration-service",
            "portMappings":[
                {
                    "containerPort":0,
                    "hostPort":31888,
                    "servicePort":0,
                    "protocol":"tcp",
                    "name":"http"
                }
            ]
        }
    },
    "env":{
        "spring.profiles.active":"dev",
        "ALL_SERVICE_RUN_HOST":"10.100.134.3",
        "EUREKA_SERVER_PORT":"8761"
    }
}








{
  "id": "mysql",
  "cpus": 0.5,
  "mem": 64.0,
  "instances": 1,
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "ppc64le/mysql",
      "network": "BRIDGE",
      "portMappings": [
        { "containerPort": 3306, "hostPort": 0, "servicePort": 0, "protocol": "tcp" }
      ]
    }
  },
  "env": {
     "MYSQL_ROOT_PASSWORD" : "password",
     "MYSQL_USER" : "test",
     "MYSQL_PASSWORD" : "test",
     "MYSQL_DB" : "BucketList"
   }
}









